{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=ltr align=center>\n",
    "    <font color=0F5298 size=7>Neurosymbolic VQA Program Generator</font><br>\n",
    "    <br>\n",
    "    <font color=D2691E size=5>Part 1: Supervised Training (Seq2Seq)</font><br>\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Goal: Supervised Training**\n",
    "\n",
    "Our first approach is **supervised learning**, also known as \"behavioral cloning.\" The goal is to train a sequence-to-sequence (Seq2Seq) model to imitate the ground-truth data perfectly.\n",
    "\n",
    "The model will take a tokenized question as input and be trained to output the *exact* tokenized program from the dataset.\n",
    "\n",
    "**Input:** `[<START>, 'Is', 'there', 'a', 'large', 'sphere', '...', '<END>']` \\\n",
    "**Target:** `[<START>, 'scene', 'filter_shape[sphere]', 'filter_size[large]', 'exist', '<END>']`\n",
    "\n",
    "We will train two different architectures for this task:\n",
    "1.  **LSTM-based Seq2Seq** with Attention\n",
    "2.  **Transformer-based Seq2Seq**\n",
    "\n",
    "All the logic is contained in `scripts/train.py`, which we will call from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import config for file paths\n",
    "import src.config as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 1: **LSTM-based Seq2Seq Model**\n",
    "\n",
    "This model uses a bidirectional LSTM as the encoder and a unidirectional LSTM with attention as the decoder. The full implementation is in `src/models/lstm_seq2seq.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting Supervised Training for LSTM ---\")\n",
    "!python ../scripts/train.py \\\n",
    "    --model_type lstm \\\n",
    "    --train_mode supervised \\\n",
    "    --model_save_path ../models/supervised_lstm.pth \\\n",
    "    --num_iters 100000 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --batch_size 64\n",
    "\n",
    "print(\"--- LSTM Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2: **Transformer-based Seq2Seq Model**\n",
    "\n",
    "This model uses the standard Transformer architecture introduced in \"Attention Is All You Need.\" The full implementation is in `src/models/transformer_seq2seq.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting Supervised Training for Transformer ---\")\n",
    "!python ../scripts/train.py \\\n",
    "    --model_type transformer \\\n",
    "    --train_mode supervised \\\n",
    "    --model_save_path ../models/supervised_transformer.pth \\\n",
    "    --num_iters 100000 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --batch_size 64\n",
    "\n",
    "print(\"--- Transformer Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: **Analyze Results**\n",
    "\n",
    "The training script saves logs to `logs/program_generator.log`. We can write a simple parser to extract the validation accuracy at each checkpoint and plot the learning curves for both models.\n",
    "\n",
    "*(Note: In a real-world scenario, you would use a dedicated tool like TensorBoard or Weights & Biases for logging, which would make plotting much easier.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_validation_accuracy(log_file_path, run_name):\n",
    "    \"\"\"A simple parser to extract validation accuracies from the log file.\"\"\"\n",
    "    accuracies = []\n",
    "    run_started = False\n",
    "    # Regex to find \"Starting training run: [run_name]\"\n",
    "    start_regex = re.compile(f\"Starting training run: {re.escape(run_name)}\")\n",
    "    # Regex to find \"Validation Accuracy: [accuracy]%\"\n",
    "    acc_regex = re.compile(r\"Validation Accuracy: (\\d+\\.\\d+)%\")\n",
    "    # Regex to find the start of any *other* run\n",
    "    other_run_regex = re.compile(r\"Starting training run:\")\n",
    "\n",
    "    try:\n",
    "        with open(log_file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if not run_started:\n",
    "                    if start_regex.search(line):\n",
    "                        run_started = True\n",
    "                else:\n",
    "                    # If we find the start of another run, stop\n",
    "                    if other_run_regex.search(line) and not start_regex.search(line):\n",
    "                        break\n",
    "                    \n",
    "                    # Find accuracy lines\n",
    "                    match = acc_regex.search(line)\n",
    "                    if match:\n",
    "                        accuracies.append(float(match.group(1)))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Log file not found at: {log_file_path}\")\n",
    "        return []\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "# --- Parse Logs ---\n",
    "log_path = os.path.join(config.LOG_DIR, \"program_generator.log\")\n",
    "\n",
    "lstm_acc = parse_validation_accuracy(log_path, \"lstm (supervised)\")\n",
    "transformer_acc = parse_validation_accuracy(log_path, \"transformer (supervised)\")\n",
    "\n",
    "print(f\"Found {len(lstm_acc)} validation points for LSTM.\")\n",
    "print(f\"Found {len(transformer_acc)} validation points for Transformer.\")\n",
    "\n",
    "# --- Plot Results ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Supervised Training: Validation Accuracy\")\n",
    "plt.xlabel(f\"Validation Step (x {config.VAL_INTERVAL} iterations)\")\n",
    "plt.ylabel(\"Program Execution Accuracy (%)\")\n",
    "\n",
    "if lstm_acc:\n",
    "    plt.plot(lstm_acc, label=\"LSTM (Supervised)\", marker='o')\n",
    "if transformer_acc:\n",
    "    plt.plot(transformer_acc, label=\"Transformer (Supervised)\", marker='x')\n",
    "\n",
    "if lstm_acc or transformer_acc:\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo data to plot. Did the training scripts run correctly and log validation?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
