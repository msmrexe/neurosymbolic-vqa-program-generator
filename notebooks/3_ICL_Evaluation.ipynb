{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=ltr align=center>\n",
    "    <font color=0F5298 size=7>Neurosymbolic VQA Program Generator</font><br>\n",
    "    <br>\n",
    "    <font color=32CD32 size=5>Part 3: In-Context Learning (ICL)</font><br>\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Goal: In-Context Learning with LLMs**\n",
    "\n",
    "Our final strategy is completely different. Instead of training a model *from scratch*, we'll leverage a large, pre-trained Large Language Model (LLM).\n",
    "\n",
    "We will use **In-Context Learning (ICL)**, which means we *prompt* the model with a few examples (\"shots\") of a question and its corresponding program. The LLM is expected to recognize the pattern and generate a correct program for a new, unseen question without any gradient updates or fine-tuning.\n",
    "\n",
    "**Example Prompt ($k=1$ shot):**\n",
    "```json\n",
    "You are an AI assistant... (system prompt)\n",
    "Question: Are there any rubber spheres?\n",
    "Program: <START> scene filter_shape[sphere] filter_material[rubber] exist <END>\n",
    "Question: How many large blue things are there?\n",
    "```\n",
    "\n",
    "We will evaluate the LLM's performance by varying the number of shots ($k$) provided in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: **Setup and Dependencies**\n",
    "\n",
    "This notebook requires the `transformers`, `torch`, and `accelerate` libraries. We also import all the necessary evaluation functions from `src.evaluation.eval_icl` and our project's config and utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import all our project modules\n",
    "import src.config as config\n",
    "import src.evaluation.eval_icl as icl_eval\n",
    "from src.executor import ClevrExecutor\n",
    "from src.vocabulary import load_vocab\n",
    "\n",
    "# Set device\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1 # 0 for cuda:0, -1 for cpu\n",
    "print(f\"Using device: {'cuda' if DEVICE == 0 else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: **Load Model and Data**\n",
    "\n",
    "We need to load:\n",
    "1.  The LLM pipeline from Hugging Face.\n",
    "2.  The raw `train_questions.json` (to sample few-shot examples from).\n",
    "3.  The raw `test_questions.json` (to evaluate on).\n",
    "4.  Our `ClevrExecutor` and `vocab` (for executor-based evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load LLM Pipeline\n",
    "print(f\"Loading model: {config.LLM_MODEL_ID}\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=config.LLM_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=DEVICE, \n",
    ")\n",
    "\n",
    "# 2. Load Raw Train Questions (for few-shot examples)\n",
    "with open(config.TRAIN_QUESTIONS_JSON, 'r') as f:\n",
    "    train_questions = json.load(f)['questions']\n",
    "\n",
    "# 3. Load Raw Test Questions (for evaluation)\n",
    "with open(config.TEST_QUESTIONS_JSON, 'r') as f:\n",
    "    test_questions = json.load(f)['questions']\n",
    "\n",
    "# 4. Load Executor and Vocab\n",
    "vocab = load_vocab(config.VOCAB_JSON_FILE)\n",
    "executor = ClevrExecutor(\n",
    "    train_scene_json=config.TRAIN_SCENES_JSON,\n",
    "    val_scene_json=config.TEST_SCENES_JSON, # Use test scenes for eval\n",
    "    vocab_json=config.VOCAB_JSON_FILE\n",
    ")\n",
    "\n",
    "print(\"\\n--- Setup Complete ---\")\n",
    "print(f\"Loaded {len(train_questions)} train questions for sampling.\")\n",
    "print(f\"Loaded {len(test_questions)} test questions for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: **Test Few-Shot Example Generation**\n",
    "\n",
    "Let's see what the `get_few_shot_examples` function does. It should randomly pick 2 examples and format them nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_context = icl_eval.get_few_shot_examples(train_questions, num_examples=2)\n",
    "print(few_shot_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: **Test Single LLM Generation**\n",
    "\n",
    "Now let's combine the prompt and a test question to see what the LLM generates. We'll also test our parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 5 shots for this test\n",
    "k_shots = 5\n",
    "base_prompt = ( \"You are an AI assistant. You must translate natural language \"\n",
    "                  \"questions into a structured sequence of program functions. \"\n",
    "                  \"The program must start with <START> and end with <END>.\")\n",
    "few_shot_context = icl_eval.get_few_shot_examples(train_questions, k_shots)\n",
    "system_prompt = f\"{base_prompt}\\n\\n{few_shot_context}\"\n",
    "\n",
    "# Get a test question\n",
    "test_question_data = test_questions[0]\n",
    "user_question = f\"Question: {test_question_data['question']}\"\n",
    "\n",
    "print(\"--- System Prompt (Truncated) ---\")\n",
    "print(system_prompt[:500] + \"...\")\n",
    "print(\"\\n--- User Question ---\")\n",
    "print(user_question)\n",
    "\n",
    "# --- Generate --- \n",
    "llm_output = icl_eval.generate_program_with_llm(pipe, system_prompt, user_question)\n",
    "print(\"\\n--- LLM Raw Output ---\")\n",
    "print(llm_output)\n",
    "\n",
    "# --- Parse ---\n",
    "parsed_program = icl_eval.parse_program_from_llm_output(llm_output)\n",
    "print(\"\\n--- Parsed Program ---\")\n",
    "print(parsed_program)\n",
    "\n",
    "# --- Ground Truth ---\n",
    "from src.utils.program_utils import list_to_str, list_to_prefix\n",
    "gt_program_str = list_to_str(list_to_prefix(test_question_data['program']))\n",
    "print(\"\\n--- Ground Truth Program (Prefix) ---\")\n",
    "print(gt_program_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: **Run Full Evaluation**\n",
    "\n",
    "Now we'll loop through our list of $k$ shots (`[0, 2, 5, 10]`) and run a full evaluation on a subset of the test data (`num_test_samples`).\n",
    "\n",
    "We will run two types of evaluation:\n",
    "\n",
    "1.  **Executor Accuracy**: We execute the LLM's program and check if the *final answer* matches the ground-truth answer. (Measures semantic correctness).\n",
    "2.  **BLEU Score**: We compare the *token sequence* of the LLM's program to the ground-truth program. (Measures syntactic similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_list = config.ICL_SHOTS_LIST\n",
    "num_samples = config.ICL_NUM_TEST_SAMPLES\n",
    "\n",
    "print(\"--- 1. Running Executor-based Accuracy Evaluation ---\")\n",
    "executor_results = icl_eval.evaluate_icl_executor(\n",
    "    pipe=pipe,\n",
    "    train_questions=train_questions,\n",
    "    test_questions=test_questions,\n",
    "    executor=executor,\n",
    "    vocab=vocab,\n",
    "    num_shots_list=shots_list,\n",
    "    num_test_samples=num_samples,\n",
    "    split='val' # Use 'val' split for executor (since we loaded test scenes as val)\n",
    ")\n",
    "print(\"\\n--- Executor Evaluation Complete ---\")\n",
    "print(executor_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 2. Running BLEU Score Evaluation ---\")\n",
    "bleu_results = icl_eval.evaluate_icl_bleu(\n",
    "    pipe=pipe,\n",
    "    train_questions=train_questions,\n",
    "    test_questions=test_questions,\n",
    "    num_shots_list=shots_list,\n",
    "    num_test_samples=num_samples\n",
    ")\n",
    "print(\"\\n--- BLEU Evaluation Complete ---\")\n",
    "print(bleu_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: **Plot Final Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "fig.suptitle('In-Context Learning (ICL) Performance vs. Number of Shots')\n",
    "\n",
    "# --- Executor Accuracy Plot ---\n",
    "ax1.set_title(\"Executor-Based Accuracy\")\n",
    "ax1.set_xlabel(\"Number of Shots in Prompt (k)\")\n",
    "ax1.set_ylabel(\"Program Execution Accuracy (%)\")\n",
    "if executor_results:\n",
    "    shots = list(executor_results.keys())\n",
    "    accuracies = [v * 100 for v in executor_results.values()]\n",
    "    ax1.plot(shots, accuracies, marker='o', color='b')\n",
    "    ax1.set_xticks(shots)\n",
    "ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# --- BLEU Score Plot ---\n",
    "ax2.set_title(\"BLEU Score Similarity\")\n",
    "ax2.set_xlabel(\"Number of Shots in Prompt (k)\")\n",
    "ax2.set_ylabel(\"Average BLEU Score\")\n",
    "if bleu_results:\n",
    "    shots = list(bleu_results.keys())\n",
    "    scores = list(bleu_results.values())\n",
    "    ax2.plot(shots, scores, marker='x', color='g')\n",
    "    ax2.set_xticks(shots)\n",
    "ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
